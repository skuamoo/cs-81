{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaji_ragupathi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# Sammon Projection\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "f = open('shakespeare_scenes.txt')\n",
    "shk = f.read()\n",
    "shakespeare_scenes = eval(shk)\n",
    "f.close()\n",
    "\n",
    "#Load Shakespeare acts\n",
    "f = open('shakespeare_acts.txt')\n",
    "shk = f.read()\n",
    "shakespeare_acts = eval(shk)\n",
    "f.close()\n",
    "\n",
    "play = \"All Plays\"\n",
    "\n",
    "title = []\n",
    "col_list = ['Title','Act','Scene', 'Text']\n",
    "\n",
    "scene_text_corpus = []\n",
    "scene_text_col = ['Scene','Text']\n",
    "act_text_corpus = []\n",
    "act_text_col = ['Act','Text']\n",
    "\n",
    "def textScatter(caption,inX,inY,t,colDict,c=[]):\n",
    "    ax=plt.axes()\n",
    "    ax.scatter(inX,inY)\n",
    "    for i,j in enumerate(zip(inX,inY)):\n",
    "        if any(c):                      \n",
    "            ax.text(j[0], j[1], t[i], color=colDict[c[i]])\n",
    "        else:                       \n",
    "            ax.text(j[0], j[1], t[i], color=\"blue\")\n",
    "    plt.title(\"Sammon Projection: \" + caption)\n",
    "    plt.show()\n",
    "\n",
    "for element in shakespeare_scenes:\n",
    "    #if(element['title'] == play):\n",
    "        title.append([element['title'], element['act'], element['scene'], element['text']])\n",
    "        scene_text_corpus.append([element['scene'],element['text']])\n",
    "\n",
    "for element in shakespeare_acts:\n",
    "    #if(element['title'] == play):\n",
    "        act_text_corpus.append([element['act'],element['text']])\n",
    "        \n",
    "# convert to dataframes\n",
    "df = pd.DataFrame(title,columns=col_list)\n",
    "scene_df_text = pd.DataFrame(scene_text_corpus,columns=scene_text_col)\n",
    "act_df_text = pd.DataFrame(act_text_corpus,columns=act_text_col)\n",
    "\n",
    "# convert text data into feature vectors\n",
    "scene_convert_features = [dict(r.iteritems()) for _, r in scene_df_text.iterrows()]\n",
    "act_convert_features = [dict(r.iteritems()) for _, r in act_df_text.iterrows()]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "scene_sparse = vectorizer.fit_transform(scene_convert_features)\n",
    "act_sparse = vectorizer.fit_transform(act_convert_features)\n",
    "\n",
    "scene_vector_array = scene_sparse.toarray()\n",
    "act_vector_array = act_sparse.toarray()\n",
    "\n",
    "#convert array to dataframe\n",
    "scene_vec_df = pd.DataFrame(scene_vector_array)\n",
    "act_vec_df = pd.DataFrame(act_vector_array)\n",
    "scene_len = scene_vec_df.shape[0]-1\n",
    "act_len = act_vec_df.shape[0]-1\n",
    "\n",
    "mds = manifold.MDS(n_components=2,max_iter=3000,eps=1e-9,dissimilarity=\"precomputed\")\n",
    "scene_similarities = euclidean_distances(scene_vec_df.ix[:,0:scene_len])\n",
    "act_similarities = euclidean_distances(act_vec_df.ix[:,0:act_len])\n",
    "\n",
    "scene_points = mds.fit_transform(scene_similarities)\n",
    "act_points = mds.fit_transform(act_similarities)\n",
    "\n",
    "colDict = {\"act0\": \"black\", \"act1\":\"red\", \"act2\":\"orange\", \"act3\":\"green\", \"act4\":\"violet\", \"act5\":\"blue\", \"act6\":\"indigo\", \"act7\":\"lime\", \"act8\":\"brown\", \"act9\":\"gray\", \"act10\":\"purple\"}\n",
    "\n",
    "# act text projection\n",
    "textScatter(\"'\"+str(play)+ \"' - By Acts (using DictVectorizer)\", act_points[:, 0] , \\\n",
    "            act_points[:, 1], df.ix[:,0]+str(\"act\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,2])\n",
    "\n",
    "# scene text projection\n",
    "textScatter(\"'\"+str(play)+ \"' By Scenes (using DictVectorizer)\", scene_points[:, 0] , scene_points[:, 1], \\\n",
    "            df.ix[:,0]+str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,1])\n",
    "\n",
    "\n",
    "# Create Sammon Projection using word frequency\n",
    "\n",
    "#Create baseline of all text in all plays\n",
    "all_acts_text = []\n",
    "all_acts_lines = []\n",
    "for act in shakespeare_acts:\n",
    "    all_acts_text.append(act['text'])\n",
    "all_text = ' '.join(all_acts_text)\n",
    "all_lines = ' '.join(all_acts_lines)\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "all_freq = nltk.FreqDist(all_tokens)\n",
    "#Get top 20 most frequent terms across all plays\n",
    "vocabulary = [item[0] for item in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "\n",
    "def word_freq(data,actscene):\n",
    "    text_array = []\n",
    "    \n",
    "    for element in data:\n",
    "#        if(element['title'] == play):\n",
    "            if(actscene==\"act\"):\n",
    "                text_array.append([element['act'],element['text']])\n",
    "            if(actscene==\"scene\"):\n",
    "                text_array.append([element['scene'],element['text']])\n",
    "\n",
    "    if(actscene==\"act\"):\n",
    "        text_col = ['Act', 'Text']\n",
    "    if(actscene==\"scene\"):\n",
    "        text_col = ['Scene', 'Text']\n",
    "    \n",
    "    df_text = pd.DataFrame(text_array,columns=text_col)\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary, tokenizer=nltk.word_tokenize)\n",
    "    freq_vec = vectorizer.fit_transform(df_text['Text']).toarray().astype(np.float64)\n",
    "\n",
    "    return freq_vec\n",
    "\n",
    "scene_freq_vec = word_freq(shakespeare_scenes,\"scene\")\n",
    "act_freq_vec = word_freq(shakespeare_acts,\"act\")\n",
    "\n",
    "scene_freq_df = pd.DataFrame(scene_freq_vec)\n",
    "act_freq_df = pd.DataFrame(act_freq_vec)\n",
    "scene_freq_len = scene_freq_df.shape[0]-1\n",
    "act_freq_len = act_freq_df.shape[0]-1\n",
    "\n",
    "mds = manifold.MDS(n_components=2,max_iter=3000,eps=1e-9,dissimilarity=\"precomputed\")\n",
    "scene_freq_sim = euclidean_distances(scene_freq_df.ix[:,0:scene_freq_len])\n",
    "act_freq_sim = euclidean_distances(act_freq_df.ix[:,0:act_freq_len])\n",
    "\n",
    "scene_freq_points = mds.fit_transform(scene_freq_sim)\n",
    "act_freq_points = mds.fit_transform(act_freq_sim)\n",
    "\n",
    "colDict = {\"act0\": \"black\", \"act1\":\"red\", \"act2\":\"orange\", \"act3\":\"green\", \"act4\":\"violet\", \"act5\":\"blue\", \"act6\":\"indigo\", \"act7\":\"lime\", \"act8\":\"brown\", \"act9\":\"gray\", \"act10\":\"purple\"}\n",
    "\n",
    "# act text projection\n",
    "textScatter(\"'\"+str(play)+ \"' - By Acts (using Word Frequency)\", act_freq_points[:, 0] , \\\n",
    "            act_freq_points[:, 1], df.ix[:,0]+str(\"act\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,2])\n",
    "\n",
    "# scene text projection\n",
    "textScatter(\"'\"+str(play)+ \"' - By Scenes (using Word Frequency)\", scene_freq_points[:, 0] , \\\n",
    "            scene_freq_points[:, 1], df.ix[:,0]+str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaji_ragupathi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# Create Sammon Project using Structural features\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "f = open('shakespeare_scenes.txt')\n",
    "shk = f.read()\n",
    "shakespeare_scenes = eval(shk)\n",
    "f.close()\n",
    "\n",
    "#Load Shakespeare acts\n",
    "f = open('shakespeare_acts.txt')\n",
    "shk = f.read()\n",
    "shakespeare_acts = eval(shk)\n",
    "f.close()\n",
    "\n",
    "def textScatter(caption,inX,inY,t,colDict,c=[]):\n",
    "    ax=plt.axes()\n",
    "    ax.scatter(inX,inY)\n",
    "    for i,j in enumerate(zip(inX,inY)):\n",
    "        if any(c):                      \n",
    "            ax.text(j[0], j[1], t[i], color=colDict[c[i]])\n",
    "        else:                       \n",
    "            ax.text(j[0], j[1], t[i], color=\"blue\")\n",
    "    plt.title(\"Sammon Projection: \" + caption)\n",
    "    plt.show()\n",
    "    \n",
    "def structural(data):\n",
    "    elements_text = [element['text'] for element in data]\n",
    "    elements_lines = [element['lines'] for element in data]\n",
    "    elements_count = len(data)\n",
    "    \n",
    "    features = np.zeros((elements_count, 11), np.float64)\n",
    "    for i, element in enumerate(elements_lines):\n",
    "        text = ' '.join(element)\n",
    "        lines = element\n",
    "        lines_count = len(lines)\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        sentences = nltk.data.load('tokenizers/punkt/english.pickle').tokenize(text.lower())\n",
    "        sentences_count = len(sentences)\n",
    "        words = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "        words_unique = list(set(words))        \n",
    "        words_line_counts = [len(nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(line.lower())) for line in lines]        \n",
    "        words_sent_counts = [len(nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence.lower())) for sentence in sentences]\n",
    "\n",
    "        #Calculate features\n",
    "        mean_word_freq = float(len(words))/float(len(words_unique))\n",
    "        mean_words_line = np.mean(words_line_counts)\n",
    "        std_words_line = np.std(words_line_counts)\n",
    "        mean_words_sent = np.mean(words_sent_counts)\n",
    "        std_words_sent = np.std(words_sent_counts)\n",
    "        mean_commas_line = float(tokens.count(\",\"))/float(lines_count)\n",
    "        mean_commas_sent = float(tokens.count(\",\"))/float(sentences_count)\n",
    "        mean_colons_line = float(tokens.count(\":\"))/float(lines_count)\n",
    "        mean_colons_sent = float(tokens.count(\":\"))/float(sentences_count)\n",
    "        mean_scolons_line = float(tokens.count(\";\"))/float(lines_count)\n",
    "        mean_scolons_sent = float(tokens.count(\";\"))/float(sentences_count)\n",
    "        \n",
    "        #Assign features to matrix\n",
    "        features[i,0] = mean_word_freq\n",
    "        features[i,1] = mean_words_line\n",
    "        features[i,2] = std_words_line\n",
    "        features[i,3] = mean_words_sent\n",
    "        features[i,4] = std_words_sent\n",
    "        features[i,5] = mean_commas_line\n",
    "        features[i,6] = mean_commas_sent\n",
    "        features[i,7] = mean_colons_line\n",
    "        features[i,8] = mean_colons_sent\n",
    "        features[i,9] = mean_scolons_line\n",
    "        features[i,10] = mean_scolons_sent\n",
    "        \n",
    "    return features\n",
    "\n",
    "scene_struc_vec = structural(shakespeare_scenes)\n",
    "act_struc_vec = structural(shakespeare_acts)\n",
    "\n",
    "scene_struc_df = pd.DataFrame(scene_struc_vec)\n",
    "act_struc_df = pd.DataFrame(act_struc_vec)\n",
    "scene_struc_len = scene_struc_df.shape[0]-1\n",
    "act_struc_len = act_struc_df.shape[0]-1\n",
    "\n",
    "mds = manifold.MDS(n_components=2,max_iter=3000,eps=1e-9,dissimilarity=\"precomputed\")\n",
    "scene_struc_sim = euclidean_distances(scene_struc_df.ix[:,0:scene_struc_len])\n",
    "act_struc_sim = euclidean_distances(act_freq_df.ix[:,0:act_struc_len])\n",
    "\n",
    "scene_struc_points = mds.fit_transform(scene_struc_sim)\n",
    "act_struc_points = mds.fit_transform(act_struc_sim)\n",
    "\n",
    "colDict = {\"act0\": \"black\", \"act1\":\"red\", \"act2\":\"orange\", \"act3\":\"green\", \"act4\":\"violet\", \"act5\":\"blue\", \"act6\":\"indigo\", \"act7\":\"lime\", \"act8\":\"brown\", \"act9\":\"gray\", \"act10\":\"purple\"}\n",
    "\n",
    "# act text projection\n",
    "textScatter(\"'\"+str(play)+ \"' - By Acts (using Structural Features)\", act_struc_points[:, 0] , \\\n",
    "            act_struc_points[:, 1], df.ix[:,0]+str(\"act\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,2])\n",
    "\n",
    "# scene text projection\n",
    "textScatter(\"'\"+str(play)+ \"' - By Scenes (using Structural Features)\", scene_struc_points[:, 0] , \\\n",
    "            scene_struc_points[:, 1], df.ix[:,0]+str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, str(\"act\")+df.ix[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Title:  All's Well That Ends Well  Act:  1  Scene:  1\n",
      "1  Title:  All's Well That Ends Well  Act:  1  Scene:  2\n",
      "2  Title:  All's Well That Ends Well  Act:  1  Scene:  3\n",
      "3  Title:  All's Well That Ends Well  Act:  2  Scene:  1\n",
      "4  Title:  All's Well That Ends Well  Act:  2  Scene:  2\n",
      "5  Title:  All's Well That Ends Well  Act:  2  Scene:  3\n",
      "6  Title:  All's Well That Ends Well  Act:  2  Scene:  4\n",
      "7  Title:  All's Well That Ends Well  Act:  2  Scene:  5\n",
      "8  Title:  All's Well That Ends Well  Act:  3  Scene:  1\n",
      "9  Title:  All's Well That Ends Well  Act:  3  Scene:  2\n",
      "10  Title:  All's Well That Ends Well  Act:  3  Scene:  3\n",
      "11  Title:  All's Well That Ends Well  Act:  3  Scene:  4\n",
      "12  Title:  All's Well That Ends Well  Act:  3  Scene:  5\n",
      "13  Title:  All's Well That Ends Well  Act:  3  Scene:  6\n",
      "14  Title:  All's Well That Ends Well  Act:  3  Scene:  7\n",
      "15  Title:  All's Well That Ends Well  Act:  4  Scene:  1\n",
      "16  Title:  All's Well That Ends Well  Act:  4  Scene:  2\n",
      "17  Title:  All's Well That Ends Well  Act:  4  Scene:  3\n",
      "18  Title:  All's Well That Ends Well  Act:  4  Scene:  4\n",
      "19  Title:  All's Well That Ends Well  Act:  4  Scene:  5\n",
      "20  Title:  All's Well That Ends Well  Act:  5  Scene:  1\n",
      "21  Title:  All's Well That Ends Well  Act:  5  Scene:  2\n",
      "\n",
      "\n",
      "There are 6 clusters\n",
      "cluster 1 is [17 18 19 20 21]\n",
      "cluster 2 is [3 4 5]\n",
      "cluster 3 is [13 14 15]\n",
      "cluster 4 is [11 12]\n",
      "cluster 5 is [ 8  9 10]\n",
      "cluster 6 is [ 0  1  2  6  7 16]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "\n",
    "import scipy.cluster.hierarchy as hc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "f = open('shakespeare.txt')\n",
    "shk = f.read()\n",
    "shakespeare = eval(shk)\n",
    "\n",
    "title = []\n",
    "text_corpus = []\n",
    "col_list = ['Title','Act','Scene']\n",
    "text_col = ['Scene','Text']\n",
    "\n",
    "for element in shakespeare:\n",
    "    if(element['title'] == \"All's Well That Ends Well\"):\n",
    "        title.append([element['title'], element['act'], element['scene']])\n",
    "        text_corpus.append([element['scene'],element['text']])\n",
    "\n",
    "# convert to dataframes\n",
    "df_text = pd.DataFrame(text_corpus,columns=text_col)\n",
    "df = pd.DataFrame(title,columns=col_list)\n",
    "\n",
    "# print data frame\n",
    "counter = df.shape[0]\n",
    "for j in range(0,counter-1):\n",
    "    print(j, \" Title: \", df['Title'][j], \" Act: \", df['Act'][j], \" Scene: \", df['Scene'][j])\n",
    "\n",
    "# convert text data into feature vectors\n",
    "convert_features = [dict(r.iteritems()) for _, r in df_text.iterrows()]\n",
    "vectorizer = DictVectorizer()\n",
    "vectorized_sparse = vectorizer.fit_transform(convert_features)\n",
    "vectorized_array = vectorized_sparse.toarray()\n",
    "\n",
    "# create hirarchical clustering using the converted feature vectors\n",
    "Z = linkage(vectorized_array,'ward')\n",
    "\n",
    "# draw dendogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Index of Scenes')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    p=6,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show() \n",
    "\n",
    "#Elbow method to find the number of clusters\n",
    "last = vectorized_array[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.title(\"Elbow Chart\")\n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "\n",
    "def cluster_indices(cluster_assignments):\n",
    "    n = cluster_assignments.max()\n",
    "    indices = []\n",
    "    for cluster_number in range(1, n + 1):\n",
    "        indices.append(np.where(cluster_assignments == cluster_number)[0])\n",
    "    return indices\n",
    "\n",
    "#clusters = fcluster(Z,k,criterion='maxclust')\n",
    "clusters = fclusterdata(Z,1.0)\n",
    "num_clusters = clusters.max()\n",
    "\n",
    "# print restults\n",
    "print(\"\\n\")\n",
    "print(\"There are %d clusters\" % num_clusters)\n",
    "indices = cluster_indices(clusters)\n",
    "for k, ind in enumerate(indices):\n",
    "    print (\"cluster\", k + 1, \"is\", ind)\n",
    "\n",
    "\n",
    "def second_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Index of Scenes')\n",
    "        plt.ylabel('Distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "second_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaji_ragupathi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# comment code\n",
    "\n",
    "# fig, axes23 = plt.subplots(2,3)\n",
    "\n",
    "# for method, axes in zip(['single','complete'],axes23):\n",
    "#     z = hc.linkage(vectorized_array, method=method)\n",
    "    \n",
    "    # Plotting\n",
    "#     axes[0].plot(range(1, len(z)+1), z[::-1, 2])\n",
    "#     knee = np.diff(z[::-1, 2], 2)\n",
    "#     axes[0].plot(range(2, len(z)), knee)\n",
    "    \n",
    "#     num_clust1 = knee.argmax() + 2\n",
    "#     knee[knee.argmax()] = 0\n",
    "#     num_clust2 = knee.argmax() + 2\n",
    "    \n",
    "#     axes[0].text(num_clust1, z[::-1, 2][num_clust1-1], 'possible\\n<- knee point')\n",
    "    \n",
    "#     part1 = hc.fcluster(z, num_clust1, 'maxclust')\n",
    "#     part2 = hc.fcluster(z, num_clust2, 'maxclust')\n",
    "    \n",
    "#     clr = ['#2200CC' ,'#D9007E' ,'#FF6600' ,'#FFCC00' ,'#ACE600' ,'#0099CC' ,\n",
    "#     '#8900CC' ,'#FF0000' ,'#FF9900' ,'#FFFF00' ,'#00CC01' ,'#0055CC']\n",
    "    \n",
    "#     for part, ax in zip([part1, part2], axes[1:]):\n",
    "#         for cluster in set(part):\n",
    "#             ax.scatter(vectorized_array[part == cluster, 0], vectorized_array[part == cluster, 1], \n",
    "#                        color=clr[cluster])\n",
    "            \n",
    "#     m = '\\n(method: {})'.format(method)\n",
    "#     plt.setp(axes[0], title='Screeplot{}'.format(m), xlabel='partition',\n",
    "#              ylabel='{}\\ncluster distance'.format(m))\n",
    "#     plt.setp(axes[1], title='{} Clusters'.format(num_clust1))\n",
    "#     plt.setp(axes[2], title='{} Clusters'.format(num_clust2))\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show() \n",
    "\n",
    "# print results\n",
    "# print('Title','Act','Scene','Cluster')\n",
    "# print('\\n')\n",
    "\n",
    "# for j in range(1,len(clusters)):\n",
    "#    print(\"'\",df['Title'][j-1], \"'\", df['Act'][j-1], df['Scene'][j-1], clusters[j-1])\n",
    "#    print('\\n')\n",
    "\n",
    "#def word_freq(data):\n",
    "#    if([element['title'] == \"All's Well That Ends Well\" for element in data]):\n",
    "#        elements_text = [element['text'] for element in data]\n",
    "#        elements_lines = [element['lines'] for element in data]\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "\n",
    "#        vectorizer = CountVectorizer(vocabulary=vocabulary, tokenizer=nltk.word_tokenize)\n",
    "#        freq_vec = vectorizer.fit_transform(elements_text).toarray().astype(np.float64)\n",
    "#        freq_vec /= np.c_[np.apply_along_axis(np.linalg.norm, 1, freq_vec)]\n",
    "\n",
    "#    return freq_vec\n",
    "\n",
    "#Create baseline of all text in all plays\n",
    "#all_acts_text = []\n",
    "#all_acts_lines = []\n",
    "#for act in shakespeare:\n",
    "#    all_acts_text.append(act['text'])\n",
    "#all_text = ' '.join(all_acts_text)\n",
    "#all_lines = ' '.join(all_acts_lines)\n",
    "#all_tokens = nltk.word_tokenize(all_text)\n",
    "#all_freq = nltk.FreqDist(all_tokens)\n",
    "#Get top 20 most frequent terms across all plays\n",
    "#vocabulary = [item[0] for item in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "\n",
    "#colDict = {\"All's Well That Ends Well\":\"red\", \"As You Like It\":\"orange\", \\\n",
    "#           \"The Comedy of Errors\":\"green\", \"Cymbeline\":\"violet\", \\\n",
    "#           \"Love's Labours Lost\":\"blue\"}\n",
    "\n",
    "# textScatter(points[:, 0] , points[:, 1], str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, df.ix[:,0])\n",
    "# textScatter(points[:, 0] , points[:, 1], str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, df.ix[:,0])\n",
    "\n",
    "# Hierarchical Clustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "\n",
    "import scipy.cluster.hierarchy as hc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#### FUNCTIONS ####\n",
    "def word_freq(data,actscene):\n",
    "    text_array = []\n",
    "    \n",
    "    for element in data:\n",
    "#        if(element['title'] == play):\n",
    "            if(actscene==\"act\"):\n",
    "                text_array.append([element['act'],element['text']])\n",
    "            if(actscene==\"scene\"):\n",
    "                text_array.append([element['scene'],element['text']])\n",
    "\n",
    "    if(actscene==\"act\"):\n",
    "        text_col = ['Act', 'Text']\n",
    "    if(actscene==\"scene\"):\n",
    "        text_col = ['Scene', 'Text']\n",
    "    \n",
    "    df_text = pd.DataFrame(text_array,columns=text_col)\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary, tokenizer=nltk.word_tokenize)\n",
    "    freq_vec = vectorizer.fit_transform(df_text['Text']).toarray().astype(np.float64)\n",
    "\n",
    "    return freq_vec\n",
    "\n",
    "def structural(data):\n",
    "    elements_text = [element['text'] for element in data]\n",
    "    elements_lines = [element['lines'] for element in data]\n",
    "    elements_count = len(data)\n",
    "    \n",
    "    features = np.zeros((elements_count, 11), np.float64)\n",
    "    for i, element in enumerate(elements_lines):\n",
    "        text = ' '.join(element)\n",
    "        lines = element\n",
    "        lines_count = len(lines)\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        sentences = nltk.data.load('tokenizers/punkt/english.pickle').tokenize(text.lower())\n",
    "        sentences_count = len(sentences)\n",
    "        words = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "        words_unique = list(set(words))        \n",
    "        words_line_counts = [len(nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(line.lower())) for line in lines]        \n",
    "        words_sent_counts = [len(nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence.lower())) for sentence in sentences]\n",
    "\n",
    "        #Calculate features\n",
    "        mean_word_freq = float(len(words))/float(len(words_unique))\n",
    "        mean_words_line = np.mean(words_line_counts)\n",
    "        std_words_line = np.std(words_line_counts)\n",
    "        mean_words_sent = np.mean(words_sent_counts)\n",
    "        std_words_sent = np.std(words_sent_counts)\n",
    "        mean_commas_line = float(tokens.count(\",\"))/float(lines_count)\n",
    "        mean_commas_sent = float(tokens.count(\",\"))/float(sentences_count)\n",
    "        mean_colons_line = float(tokens.count(\":\"))/float(lines_count)\n",
    "        mean_colons_sent = float(tokens.count(\":\"))/float(sentences_count)\n",
    "        mean_scolons_line = float(tokens.count(\";\"))/float(lines_count)\n",
    "        mean_scolons_sent = float(tokens.count(\";\"))/float(sentences_count)\n",
    "        \n",
    "        #Assign features to matrix\n",
    "        features[i,0] = mean_word_freq\n",
    "        features[i,1] = mean_words_line\n",
    "        features[i,2] = std_words_line\n",
    "        features[i,3] = mean_words_sent\n",
    "        features[i,4] = std_words_sent\n",
    "        features[i,5] = mean_commas_line\n",
    "        features[i,6] = mean_commas_sent\n",
    "        features[i,7] = mean_colons_line\n",
    "        features[i,8] = mean_colons_sent\n",
    "        features[i,9] = mean_scolons_line\n",
    "        features[i,10] = mean_scolons_sent\n",
    "        \n",
    "    return features\n",
    "\n",
    "# draw dendogram\n",
    "def drawdendogram(Z,vectorarr,caption,xlabel,ylabel):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title(caption)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        p=6,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "    )\n",
    "    plt.show() \n",
    "\n",
    "    #Elbow method to find the number of clusters\n",
    "    last = vectorarr[-10:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "    plt.plot(idxs, last_rev)\n",
    "    acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    acceleration_rev = acceleration[::-1]\n",
    "    plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "    plt.title(caption + \" Elbow Chart\")\n",
    "    plt.show()\n",
    "\n",
    "#### END OF FUNCTIONS ####\n",
    "\n",
    "f = open('shakespeare_scenes.txt')\n",
    "shk = f.read()\n",
    "shakespeare_scenes = eval(shk)\n",
    "f.close()\n",
    "\n",
    "#Load Shakespeare acts\n",
    "f = open('shakespeare_acts.txt')\n",
    "shk = f.read()\n",
    "shakespeare_acts = eval(shk)\n",
    "f.close()\n",
    "\n",
    "#play = \"Loves Labours Lost\"\n",
    "\n",
    "title = []\n",
    "col_list = ['Title','Act','Scene', 'Text']\n",
    "\n",
    "scene_text_corpus = []\n",
    "scene_text_col = ['Scene','Text']\n",
    "act_text_corpus = []\n",
    "act_text_col = ['Act','Text']\n",
    "\n",
    "for element in shakespeare_scenes:\n",
    "    #if(element['title'] == play):\n",
    "        title.append([element['title'], element['act'], element['scene'], element['text']])\n",
    "        scene_text_corpus.append([element['scene'],element['text']])\n",
    "\n",
    "for element in shakespeare_acts:\n",
    "    #if(element['title'] == play):\n",
    "        act_text_corpus.append([element['act'],element['text']])\n",
    "        \n",
    "# convert to dataframes\n",
    "df = pd.DataFrame(title,columns=col_list)\n",
    "scene_df_text = pd.DataFrame(scene_text_corpus,columns=scene_text_col)\n",
    "act_df_text = pd.DataFrame(act_text_corpus,columns=act_text_col)\n",
    "\n",
    "# convert text data into feature vectors using Dict Vectorizer\n",
    "scene_convert_features = [dict(r.iteritems()) for _, r in scene_df_text.iterrows()]\n",
    "act_convert_features = [dict(r.iteritems()) for _, r in act_df_text.iterrows()]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "scene_sparse = vectorizer.fit_transform(scene_convert_features)\n",
    "act_sparse = vectorizer.fit_transform(act_convert_features)\n",
    "\n",
    "scene_vector_array = scene_sparse.toarray()\n",
    "act_vector_array = act_sparse.toarray()\n",
    "#convert array to dataframe\n",
    "scene_vec_df = pd.DataFrame(scene_vector_array)\n",
    "act_vec_df = pd.DataFrame(act_vector_array)\n",
    "\n",
    "# convert text data into feature vectors using Word Frequency\n",
    "#Create baseline of all text in all plays\n",
    "all_acts_text = []\n",
    "all_acts_lines = []\n",
    "for act in shakespeare_acts:\n",
    "    all_acts_text.append(act['text'])\n",
    "all_text = ' '.join(all_acts_text)\n",
    "all_lines = ' '.join(all_acts_lines)\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "all_freq = nltk.FreqDist(all_tokens)\n",
    "#Get top 20 most frequent terms across all plays\n",
    "vocabulary = [item[0] for item in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "scene_freq_vec = word_freq(shakespeare_scenes,\"scene\")\n",
    "act_freq_vec = word_freq(shakespeare_acts,\"act\")\n",
    "scene_freq_df = pd.DataFrame(scene_freq_vec)\n",
    "act_freq_df = pd.DataFrame(act_freq_vec)\n",
    "\n",
    "# convert text data into feature vectors using Structural features\n",
    "scene_struc_vec = structural(shakespeare_scenes)\n",
    "act_struc_vec = structural(shakespeare_acts)\n",
    "scene_struc_df = pd.DataFrame(scene_struc_vec)\n",
    "act_struc_df = pd.DataFrame(act_struc_vec)\n",
    "\n",
    "# create hirarchical clustering using Dict Vectorizer feature vectors\n",
    "Z1 = linkage(scene_vec_df,'ward')\n",
    "Z2 = linkage(act_vec_df,'ward')\n",
    "\n",
    "# create hirarchical clustering using Word frequency feature vectors\n",
    "Z3 = linkage(scene_freq_df,'ward')\n",
    "Z4 = linkage(act_freq_df,'ward')\n",
    "\n",
    "# create hirarchical clustering using Structural feature vectors\n",
    "Z5 = linkage(scene_struc_df,'ward')\n",
    "Z6 = linkage(act_struc_df,'ward')\n",
    "\n",
    "# draw dendograms\n",
    "drawdendogram(Z1,scene_vector_array,\"Hierarchical Clustering-By Scenes-DictVectorizer\",\"Index Of Scenes\",\"Distance\")\n",
    "drawdendogram(Z2,act_vector_array,\"Hierarchical Clustering-By Acts-DictVectorizer\",\"Index Of Acts\",\"Distance\")\n",
    "\n",
    "drawdendogram(Z3,scene_freq_vec,\"Hierarchical Clustering-By Scenes-Word frequency\",\"Index Of Scenes\",\"Distance\")\n",
    "drawdendogram(Z4,act_freq_vec,\"Hierarchical Clustering-By Acts-Word frequency\",\"Index Of Acts\",\"Distance\")\n",
    "\n",
    "drawdendogram(Z5,scene_struc_vec,\"Hierarchical Clustering-By Scenes-Structural Features\",\"Index Of Scenes\",\"Distance\")\n",
    "drawdendogram(Z6,act_struc_vec,\"Hierarchical Clustering-By Acts-Structural Features\",\"Index Of Acts\",\"Distance\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
