{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaji_ragupathi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# Sammon Projection\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "f = open('shakespeare_scenes.txt')\n",
    "shk = f.read()\n",
    "shakespeare_scenes = eval(shk)\n",
    "f.close()\n",
    "\n",
    "#Load Shakespeare acts\n",
    "f = open('shakespeare_acts.txt')\n",
    "shk = f.read()\n",
    "shakespeare_acts = eval(shk)\n",
    "f.close()\n",
    "\n",
    "title = []\n",
    "text_corpus = []\n",
    "col_list = ['Title','Act','Scene']\n",
    "text_col = ['Scene','Text']\n",
    "\n",
    "for element in shakespeare_scenes:\n",
    "    if(element['title'] == \"All's Well That Ends Well\" or \\\n",
    "      element['title'] == \"As You Like It\" or \\\n",
    "      element['title'] == \"The Comedy of Errors\" or \\\n",
    "      element['title'] == \"Cymbeline\" or \\\n",
    "      element['title'] == \"Love's Labours Lost\"):\n",
    "        title.append([element['title'], element['act'], element['scene']])\n",
    "        text_corpus.append([element['scene'],element['text']])\n",
    "\n",
    "# convert to dataframes\n",
    "df_text = pd.DataFrame(text_corpus,columns=text_col)\n",
    "df = pd.DataFrame(title,columns=col_list)\n",
    "\n",
    "# convert text data into feature vectors\n",
    "convert_features = [dict(r.iteritems()) for _, r in df_text.iterrows()]\n",
    "vectorizer = DictVectorizer()\n",
    "vectorized_sparse = vectorizer.fit_transform(convert_features)\n",
    "vectorized_array = vectorized_sparse.toarray()\n",
    "\n",
    "#convert array to dataframe\n",
    "vec_df = pd.DataFrame(vectorized_array)\n",
    "\n",
    "mds = manifold.MDS(n_components=2,max_iter=3000,eps=1e-9,dissimilarity=\"precomputed\")\n",
    "similarities = euclidean_distances(vec_df.ix[:,0:81])\n",
    "points = mds.fit_transform(similarities)\n",
    "\n",
    "#colDict = {\"act1\":\"red\", \"act2\":\"orange\", \"act3\":\"green\", \"act4\":\"violet\", \"act5\":\"blue\"}\n",
    "colDict = {\"All's Well That Ends Well\":\"red\", \"As You Like It\":\"orange\", \\\n",
    "           \"The Comedy of Errors\":\"green\", \"Cymbeline\":\"violet\", \\\n",
    "           \"Love's Labours Lost\":\"blue\"}\n",
    "\n",
    "def textScatter(inX,inY,t,colDict,c=[]):\n",
    "    ax=plt.axes()\n",
    "    ax.scatter(inX,inY)\n",
    "    for i,j in enumerate(zip(inX,inY)):\n",
    "        if any(c):                      \n",
    "            ax.text(j[0], j[1], t[i], color=colDict[c[i]])\n",
    "        else:                       \n",
    "            ax.text(j[0], j[1], t[i], color=\"blue\")\n",
    "    plt.title(\"Sammon Projection\")\n",
    "    plt.show()\n",
    "\n",
    "textScatter(points[:, 0] , points[:, 1], str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, df.ix[:,0])\n",
    "\n",
    "# Create Sammon Projection using word frequency\n",
    "\n",
    "#Create baseline of all text in all plays\n",
    "all_acts_text = []\n",
    "all_acts_lines = []\n",
    "for act in shakespeare_acts:\n",
    "    all_acts_text.append(act['text'])\n",
    "all_text = ' '.join(all_acts_text)\n",
    "all_lines = ' '.join(all_acts_lines)\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "all_freq = nltk.FreqDist(all_tokens)\n",
    "#Get top 20 most frequent terms across all plays\n",
    "vocabulary = [item[0] for item in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)][:20]\n",
    "\n",
    "def word_freq(data):\n",
    "    text_array = []\n",
    "    \n",
    "    for element in data:\n",
    "        if(element['title'] == \"All's Well That Ends Well\" or \\\n",
    "          element['title'] == \"As You Like It\" or \\\n",
    "          element['title'] == \"The Comedy of Errors\" or \\\n",
    "          element['title'] == \"Cymbeline\" or \\\n",
    "          element['title'] == \"Love's Labours Lost\"):\n",
    "            \n",
    "            text_array.append([element['scene'],element['text']])\n",
    "            \n",
    "            #elements_lines = [element['lines'] for element in data]\n",
    "            #elements_count = len(data)\n",
    "            \n",
    "    text_col = ['Scene', 'Text']\n",
    "    df_text = pd.DataFrame(text_array,columns=text_col)\n",
    "                          \n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary, tokenizer=nltk.word_tokenize)\n",
    "    freq_vec = vectorizer.fit_transform(df_text['Text']).toarray().astype(np.float64)\n",
    "    #freq_vec /= np.c_[np.apply_along_axis(np.linalg.norm, 1, freq_vec)]\n",
    " \n",
    "    return freq_vec\n",
    "\n",
    "scene_freq_vec = word_freq(shakespeare_scenes)\n",
    "\n",
    "scene_freq_vec_df = pd.DataFrame(scene_freq_vec)\n",
    "\n",
    "mds = manifold.MDS(n_components=2,max_iter=3000,eps=1e-9,dissimilarity=\"precomputed\")\n",
    "similarities = euclidean_distances(vec_df.ix[:,0:81])\n",
    "points = mds.fit_transform(similarities)\n",
    "\n",
    "textScatter(points[:, 0] , points[:, 1], str(\"act\")+df.ix[:,1]+str(\"scene\")+df.ix[:,2], colDict, df.ix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "d = {'h':8, 'j':3, 'u':1, 'm':9}\n",
    "ds = OrderedDict(sorted(d.items(), key=lambda t: t[1], reverse=True))\n",
    "print (ds['j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Title:  All's Well That Ends Well  Act:  1  Scene:  1\n",
      "1  Title:  All's Well That Ends Well  Act:  1  Scene:  2\n",
      "2  Title:  All's Well That Ends Well  Act:  1  Scene:  3\n",
      "3  Title:  All's Well That Ends Well  Act:  2  Scene:  1\n",
      "4  Title:  All's Well That Ends Well  Act:  2  Scene:  2\n",
      "5  Title:  All's Well That Ends Well  Act:  2  Scene:  3\n",
      "6  Title:  All's Well That Ends Well  Act:  2  Scene:  4\n",
      "7  Title:  All's Well That Ends Well  Act:  2  Scene:  5\n",
      "8  Title:  All's Well That Ends Well  Act:  3  Scene:  1\n",
      "9  Title:  All's Well That Ends Well  Act:  3  Scene:  2\n",
      "10  Title:  All's Well That Ends Well  Act:  3  Scene:  3\n",
      "11  Title:  All's Well That Ends Well  Act:  3  Scene:  4\n",
      "12  Title:  All's Well That Ends Well  Act:  3  Scene:  5\n",
      "13  Title:  All's Well That Ends Well  Act:  3  Scene:  6\n",
      "14  Title:  All's Well That Ends Well  Act:  3  Scene:  7\n",
      "15  Title:  All's Well That Ends Well  Act:  4  Scene:  1\n",
      "16  Title:  All's Well That Ends Well  Act:  4  Scene:  2\n",
      "17  Title:  All's Well That Ends Well  Act:  4  Scene:  3\n",
      "18  Title:  All's Well That Ends Well  Act:  4  Scene:  4\n",
      "19  Title:  All's Well That Ends Well  Act:  4  Scene:  5\n",
      "20  Title:  All's Well That Ends Well  Act:  5  Scene:  1\n",
      "21  Title:  All's Well That Ends Well  Act:  5  Scene:  2\n",
      "\n",
      "\n",
      "There are 6 clusters\n",
      "cluster 1 is [17 18 19 20 21]\n",
      "cluster 2 is [3 4 5]\n",
      "cluster 3 is [13 14 15]\n",
      "cluster 4 is [11 12]\n",
      "cluster 5 is [ 8  9 10]\n",
      "cluster 6 is [ 0  1  2  6  7 16]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "\n",
    "import scipy.cluster.hierarchy as hc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "f = open('shakespeare.txt')\n",
    "shk = f.read()\n",
    "shakespeare = eval(shk)\n",
    "\n",
    "title = []\n",
    "text_corpus = []\n",
    "col_list = ['Title','Act','Scene']\n",
    "text_col = ['Scene','Text']\n",
    "\n",
    "for element in shakespeare:\n",
    "    if(element['title'] == \"All's Well That Ends Well\"):\n",
    "        title.append([element['title'], element['act'], element['scene']])\n",
    "        text_corpus.append([element['scene'],element['text']])\n",
    "\n",
    "# convert to dataframes\n",
    "df_text = pd.DataFrame(text_corpus,columns=text_col)\n",
    "df = pd.DataFrame(title,columns=col_list)\n",
    "\n",
    "# print data frame\n",
    "counter = df.shape[0]\n",
    "for j in range(0,counter-1):\n",
    "    print(j, \" Title: \", df['Title'][j], \" Act: \", df['Act'][j], \" Scene: \", df['Scene'][j])\n",
    "\n",
    "# convert text data into feature vectors\n",
    "convert_features = [dict(r.iteritems()) for _, r in df_text.iterrows()]\n",
    "vectorizer = DictVectorizer()\n",
    "vectorized_sparse = vectorizer.fit_transform(convert_features)\n",
    "vectorized_array = vectorized_sparse.toarray()\n",
    "\n",
    "# create hirarchical clustering using the converted feature vectors\n",
    "Z = linkage(vectorized_array,'ward')\n",
    "\n",
    "# draw dendogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Index of Scenes')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    p=6,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show() \n",
    "\n",
    "#Elbow method to find the number of clusters\n",
    "last = vectorized_array[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.title(\"Elbow Chart\")\n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "\n",
    "def cluster_indices(cluster_assignments):\n",
    "    n = cluster_assignments.max()\n",
    "    indices = []\n",
    "    for cluster_number in range(1, n + 1):\n",
    "        indices.append(np.where(cluster_assignments == cluster_number)[0])\n",
    "    return indices\n",
    "\n",
    "#clusters = fcluster(Z,k,criterion='maxclust')\n",
    "clusters = fclusterdata(Z,1.0)\n",
    "num_clusters = clusters.max()\n",
    "\n",
    "# print restults\n",
    "print(\"\\n\")\n",
    "print(\"There are %d clusters\" % num_clusters)\n",
    "indices = cluster_indices(clusters)\n",
    "for k, ind in enumerate(indices):\n",
    "    print (\"cluster\", k + 1, \"is\", ind)\n",
    "\n",
    "\n",
    "def second_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram')\n",
    "        plt.xlabel('Index of Scenes')\n",
    "        plt.ylabel('Distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "second_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comment code\n",
    "\n",
    "# fig, axes23 = plt.subplots(2,3)\n",
    "\n",
    "# for method, axes in zip(['single','complete'],axes23):\n",
    "#     z = hc.linkage(vectorized_array, method=method)\n",
    "    \n",
    "    # Plotting\n",
    "#     axes[0].plot(range(1, len(z)+1), z[::-1, 2])\n",
    "#     knee = np.diff(z[::-1, 2], 2)\n",
    "#     axes[0].plot(range(2, len(z)), knee)\n",
    "    \n",
    "#     num_clust1 = knee.argmax() + 2\n",
    "#     knee[knee.argmax()] = 0\n",
    "#     num_clust2 = knee.argmax() + 2\n",
    "    \n",
    "#     axes[0].text(num_clust1, z[::-1, 2][num_clust1-1], 'possible\\n<- knee point')\n",
    "    \n",
    "#     part1 = hc.fcluster(z, num_clust1, 'maxclust')\n",
    "#     part2 = hc.fcluster(z, num_clust2, 'maxclust')\n",
    "    \n",
    "#     clr = ['#2200CC' ,'#D9007E' ,'#FF6600' ,'#FFCC00' ,'#ACE600' ,'#0099CC' ,\n",
    "#     '#8900CC' ,'#FF0000' ,'#FF9900' ,'#FFFF00' ,'#00CC01' ,'#0055CC']\n",
    "    \n",
    "#     for part, ax in zip([part1, part2], axes[1:]):\n",
    "#         for cluster in set(part):\n",
    "#             ax.scatter(vectorized_array[part == cluster, 0], vectorized_array[part == cluster, 1], \n",
    "#                        color=clr[cluster])\n",
    "            \n",
    "#     m = '\\n(method: {})'.format(method)\n",
    "#     plt.setp(axes[0], title='Screeplot{}'.format(m), xlabel='partition',\n",
    "#              ylabel='{}\\ncluster distance'.format(m))\n",
    "#     plt.setp(axes[1], title='{} Clusters'.format(num_clust1))\n",
    "#     plt.setp(axes[2], title='{} Clusters'.format(num_clust2))\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show() \n",
    "\n",
    "# print results\n",
    "# print('Title','Act','Scene','Cluster')\n",
    "# print('\\n')\n",
    "\n",
    "# for j in range(1,len(clusters)):\n",
    "#    print(\"'\",df['Title'][j-1], \"'\", df['Act'][j-1], df['Scene'][j-1], clusters[j-1])\n",
    "#    print('\\n')\n",
    "\n",
    "#def word_freq(data):\n",
    "#    if([element['title'] == \"All's Well That Ends Well\" for element in data]):\n",
    "#        elements_text = [element['text'] for element in data]\n",
    "#        elements_lines = [element['lines'] for element in data]\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "#        elements_count = len(data)\n",
    "\n",
    "#        vectorizer = CountVectorizer(vocabulary=vocabulary, tokenizer=nltk.word_tokenize)\n",
    "#        freq_vec = vectorizer.fit_transform(elements_text).toarray().astype(np.float64)\n",
    "#        freq_vec /= np.c_[np.apply_along_axis(np.linalg.norm, 1, freq_vec)]\n",
    "\n",
    "#    return freq_vec\n",
    "\n",
    "#Create baseline of all text in all plays\n",
    "#all_acts_text = []\n",
    "#all_acts_lines = []\n",
    "#for act in shakespeare:\n",
    "#    all_acts_text.append(act['text'])\n",
    "#all_text = ' '.join(all_acts_text)\n",
    "#all_lines = ' '.join(all_acts_lines)\n",
    "#all_tokens = nltk.word_tokenize(all_text)\n",
    "#all_freq = nltk.FreqDist(all_tokens)\n",
    "#Get top 20 most frequent terms across all plays\n",
    "#vocabulary = [item[0] for item in sorted(all_freq.items(), key=lambda x: x[1], reverse=True)][:20]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
